{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch introduction\n",
    "The fundamental building block in PyTorch is *tensors*. A PyTorch Tensor is conceptually identical to a numpy array. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors.\n",
    "\n",
    "Unlike NumPy, Tensors can utilize GPUs to accelerate numeric computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0224,  0.2589, -0.9608],\n",
      "        [-0.5659,  0.1118,  2.2862]])\n",
      "tensor([[2.0224, 1.2589, 0.0392],\n",
      "        [0.4341, 1.1118, 3.2862]])\n",
      "tensor([[ 2.0354, -2.7462],\n",
      "        [-2.7462,  5.5594]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')     # Run on GPU\n",
    "\n",
    "# Create a random matrix\n",
    "x = torch.randn(2,3, device=device)\n",
    "print(x)\n",
    "print(x+1)\n",
    "print(torch.matmul(x,x.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd\n",
    "Manually implementing the backward pass for large networks can quickly get complet. \n",
    "With *automatic differentiation*, this can be done automatically using `autograd` in PyTorch. The forward pass in a network will define a computational graph, where nodes will be Tensors and edges will be functions that produce output Tensors from the input Tensors. \n",
    "\n",
    "The only thing we need to do it specifying `requires_grad=True` when constructing a Tensor.\n",
    "\n",
    "With `x` being a tensor with `requires_grad=True`, after backpropagation `x.grad` will be a Tensor holding the gradient of `x` with respect to some scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: tensor([1.]) \n",
      "Predicted: tensor([2.], grad_fn=<AddBackward0>)\n",
      "Loss: 1.0\n",
      "Gradient b: tensor([2.])\n",
      "Gradient w: tensor([0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0.1], requires_grad=True)\n",
    "b = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([0.0])\n",
    "y = torch.tensor([1.0])\n",
    "\n",
    "# Forward pass: compute predicted y using operations on Tensors. Since w and\n",
    "# b have requires_grad=True, operations involving these Tensors will cause\n",
    "# PyTorch to build a computational graph, allowing automatic computation of\n",
    "# gradients. Since we are no longer implementing the backward pass by hand we\n",
    "# don't need to keep references to intermediate values.\n",
    "\n",
    "y_pred = w*x+b\n",
    "print(f'True label: {y}', f'\\nPredicted: {y_pred}')\n",
    "loss = (y_pred - y).pow(2)\n",
    "\n",
    "print(f'Loss: {loss.item()}')\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(f'Gradient b: {b.grad}')\n",
    "print(f'Gradient w: {w.grad}')\n",
    "\n",
    "# Manually zero the gradients after running the backward pass\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: New autograd functions\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclasss of `torch.autograd.Function` and implemnting the `forward` and `backward` functions. \n",
    "\n",
    "Example of implementing our own ReLU function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        x_out = x.clamp(min=0)\n",
    "        print(f'MyReLU forward {x} -> {x_out}')\n",
    "        return x_out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x<0] = 0\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyReLU forward tensor([2.], grad_fn=<AddBackward0>) -> tensor([2.])\n",
      "True label: tensor([1.]) \n",
      "Predicted: tensor([2.], grad_fn=<MyReLUBackward>)\n",
      "Loss: 1.0\n",
      "Gradient b: tensor([2.])\n",
      "Gradient w: tensor([0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0.1], requires_grad=True)\n",
    "b = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([0.0])\n",
    "y = torch.tensor([1.0])\n",
    "\n",
    "y_pred = MyReLU.apply(w*x+b)\n",
    "print(f'True label: {y}', f'\\nPredicted: {y_pred}')\n",
    "loss = (y_pred - y).pow(2)\n",
    "\n",
    "print(f'Loss: {loss.item()}')\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(f'Gradient b: {b.grad}')\n",
    "print(f'Gradient w: {w.grad}')\n",
    "\n",
    "# Manually zero the gradients after running the backward pass\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: nn\n",
    "For large neural networks, raw autograd can be a bit too low-level. \n",
    "\n",
    "When building neural networks we usually arrange the computation into layers, wheras some have learnable parameters to be optimized during learning. \n",
    "\n",
    "In PyTorch the `nn` package defines a set of *Modules* which are roughly equivalent to neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415.1668395996094\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "y_pred = model(x) \n",
    "\n",
    "loss = loss_fn(y_pred, y)\n",
    "print(loss.item())\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "# we can access its data and gradients like we did before.\n",
    "# Example of parameter update\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Custom nn Modules\n",
    "You can define your own Modules by subclassing `nn.Module` and defining the `forward` pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary (differentiable) operations on Tensors.\n",
    "    \"\"\"\n",
    "    h_relu = self.linear1(x).clamp(min=0)\n",
    "    y_pred = self.linear2(h_relu)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device = device)\n",
    "y = torch.randn(N, D_out, device = device)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define the optimization algorithm to be used (Stochastic Gradient Descent):\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)\n",
    "\n",
    "# Compute and print loss\n",
    "loss = loss_fn(y_pred, y)\n",
    "print(loss.item())\n",
    "\n",
    "# Zero gradients, perform a backward pass, and update the weights.\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
